{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOK-6wmkFuv6"
      },
      "source": [
        "# AI534 IA 4: Exploring word embeddings\n",
        "\n",
        "**Overview**:\n",
        "In this assignment, you will explore pre-trained word embeddings and use them to better understand textual data and improve downstream models.\n",
        "\n",
        "Specifically, you will:\n",
        "* Build a compact dataset of semantically related words using GloVe embeddings\n",
        "* Cluster the word embeddings and compare different clustering metrics\n",
        "* Visualize clusters using PCA and t-SNE\n",
        "* (Bonus) Incorporate word embeddings or word-cluster representations into sentiment classification, building on your work in IA3\n",
        "\n",
        "**What you need to submit**:\n",
        "1. Your completed notebook in `.ipynb` format.\n",
        "2. A PDF report that includes all code outputs and figures.\n",
        "\n",
        "If some figures or outputs are missing in the PDF due to rendering/scrolling issues, manually add them (e.g., by inserting screenshots or exporting via another tool) so your PDF has all required results.\n",
        "\n",
        "We have supplied auxiliary code for working with word embeddings. It is advisable to retain this code in its original form. Should you opt to modify this helper code, please ensure that your alterations are accompanied by comprehensive comments. This will facilitate your TA's understanding of the modifications and the rationale behind them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfC82i5Bw_Cm"
      },
      "outputs": [],
      "source": [
        "!pip install nbconvert > /dev/null 2>&1\n",
        "!pip install pdfkit > /dev/null 2>&1\n",
        "!apt-get install -y wkhtmltopdf > /dev/null 2>&1\n",
        "import os\n",
        "import pdfkit\n",
        "import contextlib\n",
        "import sys\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "# add more imports if necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z0sSMySHB-Z"
      },
      "source": [
        "# Background: Data and Word Embeddings\n",
        "\n",
        "For the first part of this assignment, you will use GloVe, a word embedding model pre-trained on large corpora of unlabeled text. There are many other word embedding methods (see, for example, this overview: https://www.turing.com/kb/guide-on-word-embeddings-in-nlp), but here we will use GloVe embeddings (https://nlp.stanford.edu/projects/glove/).\n",
        "\n",
        "Conceptually, in Part 1 you will treat words as the objects of interest and their GloVe embeddings as feature vectors describing them. These embeddings place words as points in a continuous ‚Äúsemantic‚Äù space, where semantically similar terms (such as *good* and *nice*) are located close to one another.\n",
        "\n",
        "To avoid dealing with the full GloVe vocabulary, we provide a reduced file `GloVe_Embedder_data.txt` on Canvas. This file contains a subset of words from the IA3 sentiment dataset that also appear in the full GloVe vocabulary, along with their embeddings. Download this file and place it in the same Google Drive directory as the rest of your AI534 data so the notebook can access it.\n",
        "\n",
        "If a word you encounter is not present in this embedding file, it will simply be treated as an unknown token by the helper code. This is expected behavior and does not require any special handling from you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6bo32Qeb1kY",
        "outputId": "46243909-ae7d-467a-9d3c-eab85da48d94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "EMBEDDING_PATH = '/content/gdrive/My Drive/AI534/GloVe_Embedder_data.txt' #please do not modify this path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper class and functions --- Please leave as is.\n",
        "# If you need to modify this block, please clearly indicate your change by providing detailed comments.\n",
        "#\n",
        "# Loads GloVe embeddings from a designated file location.\n",
        "#\n",
        "# Invoked via:\n",
        "# ge = GloVe_Embedder(path_to_embeddings)\n",
        "#\n",
        "# Embed single word via:\n",
        "# embed = ge.embed_str(word)\n",
        "#\n",
        "# Embed a list of words via:\n",
        "# embeds = ge.embed_list(word_list)\n",
        "#\n",
        "# Find k nearest neighbors of word via:\n",
        "# ge.find_k_nearest(word, k)\n",
        "#\n",
        "# Save vocabulary to file via:\n",
        "# ge.save_to_file(path_to_file)\n",
        "\n",
        "class GloVe_Embedder:\n",
        "    def __init__(self, path):\n",
        "        self.embedding_dict = {}\n",
        "        self.embedding_array = []\n",
        "        self.unk_emb = 0\n",
        "        # Adapted from https://stackoverflow.com/questions/37793118/load-pretrained-GloVe-vectors-in-python\n",
        "        with open(path,'r') as f:\n",
        "            for line in f:\n",
        "                split_line = line.split()\n",
        "                word = split_line[0]\n",
        "                embedding = np.array(split_line[1:], dtype=np.float64)\n",
        "                self.embedding_dict[word] = embedding\n",
        "                self.embedding_array.append(embedding.tolist())\n",
        "        self.embedding_array = np.array(self.embedding_array)\n",
        "        self.embedding_dim = len(self.embedding_array[0])\n",
        "        self.vocab_size = len(self.embedding_array)\n",
        "        self.unk_emb = np.zeros(self.embedding_dim)\n",
        "\n",
        "    # Check if the provided embedding is the unknown embedding.\n",
        "    def is_unk_embed(self, embed):\n",
        "        return np.sum((embed - self.unk_emb) ** 2) < 1e-7\n",
        "\n",
        "    # Check if the provided string is in the vocabulary.\n",
        "    def token_in_vocab(self, x):\n",
        "        if x in self.embedding_dict and not self.is_unk_embed(self.embedding_dict[x]):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # Returns the embedding for a single string and prints a warning if\n",
        "    # the string is unknown to the vocabulary.\n",
        "    #\n",
        "    # If indicate_unk is set to True, the return type will be a tuple of\n",
        "    # (numpy array, bool) with the bool indicating whether the returned\n",
        "    # embedding is the unknown embedding.\n",
        "    #\n",
        "    # If warn_unk is set to False, the method will no longer print warnings\n",
        "    # when used on unknown strings.\n",
        "    def embed_str(self, x, indicate_unk = False, warn_unk = True):\n",
        "        if self.token_in_vocab(x):\n",
        "            if indicate_unk:\n",
        "                return (self.embedding_dict[x], False)\n",
        "            else:\n",
        "                return self.embedding_dict[x]\n",
        "        else:\n",
        "            if warn_unk:\n",
        "                    print(\"Warning: provided word is not part of the vocabulary!\")\n",
        "            if indicate_unk:\n",
        "                return (self.unk_emb, True)\n",
        "            else:\n",
        "                return self.unk_emb\n",
        "\n",
        "    # Returns an array containing the embeddings of each vocabulary token in the provided list.\n",
        "    #\n",
        "    # If include_unk is set to False, the returned list will not include any unknown embeddings.\n",
        "    def embed_list(self, x, include_unk = True):\n",
        "        if include_unk:\n",
        "            embeds = [self.embed_str(word, warn_unk = False).tolist() for word in x]\n",
        "        else:\n",
        "            embeds_with_unk = [self.embed_str(word, indicate_unk=True, warn_unk = False) for word in x]\n",
        "            embeds = [e[0].tolist() for e in embeds_with_unk if not e[1]]\n",
        "            if len(embeds) == 0:\n",
        "                print(\"No known words in input:\" + str(x))\n",
        "                embeds = [self.unk_emb.tolist()]\n",
        "        return np.array(embeds)\n",
        "\n",
        "    # Finds the vocab words associated with the k nearest embeddings of the provided word.\n",
        "    # Can also accept an embedding vector in place of a string word.\n",
        "    # Return type is a nested list where each entry is a word in the vocab followed by its\n",
        "    # distance from whatever word was provided as an argument.\n",
        "    def find_k_nearest(self, word, k, warn_about_unks = True):\n",
        "        if type(word) == str:\n",
        "            word_embedding, is_unk = self.embed_str(word, indicate_unk = True)\n",
        "        else:\n",
        "            word_embedding = word\n",
        "            is_unk = False\n",
        "        if is_unk and warn_about_unks:\n",
        "            print(\"Warning: provided word is not part of the vocabulary!\")\n",
        "\n",
        "        all_distances = np.sum((self.embedding_array - word_embedding) ** 2, axis = 1) ** 0.5\n",
        "        distance_vocab_index = [[w, round(d, 5)] for w,d,i in zip(self.embedding_dict.keys(), all_distances, range(len(all_distances)))]\n",
        "        distance_vocab_index = sorted(distance_vocab_index, key = lambda x: x[1], reverse = False)\n",
        "        return distance_vocab_index[:k]\n",
        "\n",
        "    def save_to_file(self, path):\n",
        "        with open(path, 'w') as f:\n",
        "            for k in self.embedding_dict.keys():\n",
        "                embedding_str = \" \".join([str(round(s, 5)) for s in self.embedding_dict[k].tolist()])\n",
        "                string = k + \" \" + embedding_str\n",
        "                f.write(string + \"\\n\")"
      ],
      "metadata": {
        "id": "8y8aK8bHfLpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 0(10 pts) : Build your data set of words for exploration.\n",
        "In this part you will be a small data set of words and explore both clustering and dimension reduction on this data."
      ],
      "metadata": {
        "id": "u_ohaiXEVL0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üöß Task: Build your own data set of words.\n",
        "You will begin by constructing a compact dataset of words for visualization and experimentation.\n",
        "Use the following seed words as your starting point:\n",
        "\n",
        "**`flight`, `awesome`, `terrible`, `help`, `late`**\n",
        "\n",
        "For each seed word:\n",
        "\n",
        "* Retrieve the **30 nearest neighbor words** from the provided vocabulary (`GloVe_Embedder_data.txt`).\n",
        "* Use the `find_k_nearest` function (Euclidean distance) to retrieve its nearest neighbors.  \n",
        "  **Note:** `find_k_nearest` will return the seed word itself as one of the neighbors. Thus you can request **31 neighbors**, then **remove the seed word** from the result to obtain the desired **30 nearest neighbor words**.\n",
        "* Store both the retrieved words and their embeddings.\n",
        "\n",
        "If a word appears in the neighbor lists of multiple seed words, keep **one copy** of that word in your final dataset and assign it to the seed word to which it is **closest** (smallest distance).\n",
        "\n",
        "This will give you **up to 150 unique words** (fewer if there is overlap), grouped into five clusters corresponding to the five seed words.\n",
        "\n",
        "As a reference, display the 30 nearest neighbors for each seed word in a **dataframe**.\n"
      ],
      "metadata": {
        "id": "MyuztLhmV-b3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sc2kX8JxXLiI"
      },
      "outputs": [],
      "source": [
        "# Your code goes here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPW1uoyVb1ke"
      },
      "source": [
        "# Part 1 (35pts): Clustering the words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üöß (15 pts) Task 1.1: Kmeans objective as a function of $k$\n",
        "Apply the k-means clustering algorithm to your word-embedding dataset using a range of cluster counts.  \n",
        "Use `sklearn.cluster.KMeans` and keep the default settings **except** for `n_clusters`.\n",
        "\n",
        "For each value of \\(k\\) from **2 to 20**:\n",
        "\n",
        "* Fit a k-means model to your word embeddings.\n",
        "* Record the **k-means objective** (the `inertia_` attribute in scikit-learn), defined as  \n",
        "  $$\n",
        "  \\sum_{i=1}^k \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2 .\n",
        "  $$\n",
        "\n",
        "Finally, **plot the k-means objective as a function of \\(k\\)**.  \n",
        "Include clear axis labels and a descriptive title.\n"
      ],
      "metadata": {
        "id": "p4WGBxyhDklm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_hSwotTb1kg"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úçÔ∏è **Question:**\n",
        "1. Is the k-means objective strictly decreasing as \\(k\\) increases? In practice you may observe some deviations from monotonic decrease, explain why they may occur?\n",
        "\n",
        "2. Does the plotted curve provide any evidence that \\(k = 5\\) might be a reasonable number of clusters for this dataset? Discuss your reasoning."
      ],
      "metadata": {
        "id": "FY2Px_zZD6I1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here.**"
      ],
      "metadata": {
        "id": "3MCGYkIiTAaN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3VOoOIHb1ki"
      },
      "source": [
        "## üöß (20 pts) Task 1.2: evaluating your clustering against ground truth (20 pts)\n",
        "Use the original seed-word assignments as **ground-truth labels** for your dataset, and evaluate the quality of your k-means clustering results for different values of \\(k\\).\n",
        "**Reminder:** The ground-truth label for each word is the seed word whose neighbor list it came from (after resolving any overlaps using the distance rule from Task 1.1).\n",
        "For each \\(k\\) from **2 to 20**, compute the following metrics:\n",
        "\n",
        "- **Purity** (You must implement this metric yourself.)\n",
        "- **Adjusted Rand Index (ARI)**  Use `sklearn.metrics.adjusted_rand_score`.\n",
        "- **Normalized Mutual Information (NMI)** Use `sklearn.metrics.normalized_mutual_info_score`.\n",
        "\n",
        "Plot each metric as a function of \\(k\\).  \n",
        "Be sure to label axes clearly and provide legends where appropriate.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY58CFs3b1kh"
      },
      "outputs": [],
      "source": [
        "# Your code goes here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMnl71l5b1ki"
      },
      "source": [
        "##‚úçÔ∏è **Question:**\n",
        "\n",
        "1. Based on the three evaluation metrics (Purity, ARI, NMI), does \\(k = 5\\) appear to give the best scores? Comment on how each metric behaves as \\(k\\) varies, and explain why the ‚Äúbest‚Äù value of \\(k\\) may or may not align with the true number of clusters in the data.\n",
        "\n",
        "2. Suppose you want to compare **two different clustering algorithms**, each of which automatically chooses its own number of clusters (for example, one may return 5 clusters while the other returns 10). Among Purity, ARI, and NMI, which metric(s) are appropriate for comparing their performance?  Explain your reasoning‚Äîparticularly how each metric handles differences in the number of clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxuuu2jVb1ki"
      },
      "source": [
        "**Your answer goes here.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIn4qhdOftK9"
      },
      "source": [
        "# Part 2 (35 pts): Dimension reduction and visualization\n",
        "In this part, you will reduce the dimension of your data to 2-d using PCA and t-SNE and visualize them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGzsL6WRb1kb"
      },
      "source": [
        "## üöß  (15 pts) Task 2.1: apply PCA dimension reduction\n",
        "\n",
        "1. Apply **Principal Component Analysis (PCA)** to your word-embedding dataset using  \n",
        "`sklearn.decomposition.PCA` and project the embeddings into **2 dimensions**.\n",
        "\n",
        "2. Create a **2D scatter plot** of the PCA-reduced embeddings using `matplotlib`, using **different colors** to indicate the cluster associated with each seed word (i.e., the seed word from which each neighbor originated).\n",
        "4. Annotate a **selected set of words**‚Äîincluding some from well-separated regions and some from visually overlapping regions‚Äîso the plot is easier to interpret. Use the `annotate` function from `matplotlib` to label the chosen points.\n",
        "\n",
        "Your plot should include:\n",
        "* Axis labels (e.g., ‚ÄúPC1‚Äù, ‚ÄúPC2‚Äù)  \n",
        "* A legend mapping colors to seed-word clusters  \n",
        "* A descriptive title"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here."
      ],
      "metadata": {
        "id": "7M3oo-6x4Tt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úçÔ∏è **Question**: Reflection on PCA results\n",
        "Does the 2-D PCA visualization reveal **five clearly separated clusters**?  \n",
        "Describe where clusters appear well-separated and where they overlap or mix.  \n",
        "In your discussion, consider that both the underlying word embeddings **and** the PCA projection to 2-D can influence the amount of visible separation."
      ],
      "metadata": {
        "id": "ms2DArnsA4JM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcoWDhTGb1kd"
      },
      "source": [
        "**Your answer goes here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt-E4vKjb1kd"
      },
      "source": [
        "## üöß(20 pts) Task 2.2: apply t-SNE\n",
        "\n",
        "Now apply **t-SNE**, a nonlinear dimensionality-reduction method, to your word-embedding dataset.  \n",
        "Use `sklearn.manifold.TSNE` with **Euclidean distance** to project the embeddings into **2 dimensions**.\n",
        "\n",
        "t-SNE is sensitive to its **perplexity** parameter. The original authors recommend values between 5 and 50, so for this assignment you should run t-SNE using the following perplexities:\n",
        "\n",
        "**5, 10, 20, 30, 40, 50**\n",
        "\n",
        "For each chosen perplexity:\n",
        "\n",
        "1. Compute the 2-D t-SNE embedding.  \n",
        "2. Create a **2D scatter plot** using the same color scheme as in the PCA task (one color per seed-word cluster).  \n",
        "3. Annotate a **selected set of words** that help you interpret the visualization‚Äîinclude examples from **well-separated** regions and **visually overlapping** regions.  \n",
        "   Use the `annotate` function from `matplotlib` for labeling points.\n",
        "\n",
        "Your plots should include:\n",
        "* Axis labels (e.g., ‚Äút-SNE dim 1‚Äù, ‚Äút-SNE dim 2‚Äù)  \n",
        "* A legend mapping colors to seed-word clusters  \n",
        "* A descriptive title indicating the perplexity used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA_6kEnjb1kd"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úçÔ∏è **Question: Reflection on t-SNE results**\n",
        "\n",
        "Across the different perplexity settings, do the 2-D t-SNE visualizations reveal distinct clusters?\n",
        "Describe where clusters appear well separated and where they overlap or mix, and how this changes with perplexity.\n",
        "Comment on how the perplexity parameter influences the visible structure."
      ],
      "metadata": {
        "id": "nEsUnQ4dBTP8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZypJkiSb1ke"
      },
      "source": [
        " **Your answe goes here.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úçÔ∏è Question: PCA vs. t-SNE\n",
        "Compare your PCA and t-SNE visualizations.\n",
        "Which method provides clearer separation between clusters, and in what ways do their results differ?\n",
        "Discuss how PCA‚Äôs linear projection and t-SNE‚Äôs emphasis on local neighborhoods may explain the differences you observe."
      ],
      "metadata": {
        "id": "uONaii__g06M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5nD-GSab1ki"
      },
      "source": [
        "# Optional Part 3 (Up to 30 bonus pts): Using word embeddings to improve tweet classification\n",
        "\n",
        "In this bonus task, you will return to the **sentiment classification dataset** used in IA3 and explore how to improve the basic bag-of-words (BoW) representation by leveraging **word embeddings**.\n",
        "\n",
        "Your goal is to answer the following question:\n",
        "\n",
        "> **How can word embeddings be used to create a more effective representation of tweets for classification?**\n",
        "\n",
        "You should continue using classifiers covered in this course (e.g., logistic regression, naive Bayes, SVM, k-NN, tree-based models).  \n",
        "**Do not use deep learning models** for this bonus task.\n",
        "\n",
        "Because the dataset is heavily imbalanced (‚âà80% negative, ‚âà20% positive), use **Area Under the ROC Curve (AUC)** (`sklearn.metrics.roc_auc_score`) on the validation data as your primary performance metric.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üå± Seed ideas.\n",
        "\n",
        "Below are two example directions. You may explore one of these or propose your own.\n",
        "\n",
        "---\n",
        "\n",
        "### **Idea 1: Embedding-based averaged representations**\n",
        "\n",
        "Represent a tweet as the **weighted average** of its word embeddings.  \n",
        "For example:\n",
        "- Weight each word via its (normalized) tf-idf value.\n",
        "- Average the weighted embeddings to obtain a dense, low-dimensional vector.\n",
        "\n",
        "This can reduce dimensionality and potentially improve generalization.\n",
        "\n",
        "---\n",
        "\n",
        "### **Idea 2: Bag-of-word-clusters (with optional bi-clusters)**\n",
        "\n",
        "BoW treats semantically similar words as unrelated.  \n",
        "To reduce redundancy:\n",
        "\n",
        "- Cluster the vocabulary based on word embeddings (e.g., k-means).\n",
        "- Replace each word in a tweet with its **cluster ID** to form a bag-of-cluster representation.\n",
        "\n",
        "**Optional extension:**  \n",
        "Consider the benefit of bigram features in IA3, you could also consider mapping bigrams to **bi-clusters**:\n",
        "- For a bigram *(w‚ÇÅ, w‚ÇÇ)*, replace it with the pair *(cluster(w‚ÇÅ), cluster(w‚ÇÇ))*.\n",
        "- Build a bag-of-bi-clusters representation.\n",
        "\n",
        "This preserves some local structure while greatly reducing feature dimensionality.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "azxlFb3eoU9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíØ Bonus Point Structure (up to 30 points)\n",
        "Your bonus points will be awarded in three tiers:\n",
        "\n",
        "#### **Tier 1 ‚Äî Basic implementation (10 pts)**  \n",
        "Awarded if you:\n",
        "- Implement one valid idea correctly.\n",
        "- Train at least one classifier on your new representation.\n",
        "- Report training and validation AUC.\n",
        "- Provide a brief description of your approach.\n",
        "\n",
        "#### **Tier 2 ‚Äî Thoughtful exploration (10 pts)**  \n",
        "Awarded for going beyond the basics by doing **one or more** of:\n",
        "- Exploring meaningful hyperparameter variations  \n",
        "  (e.g., number of clusters, tf-idf weighting schemes, classifier choices).\n",
        "- Providing informative comparisons or plots.\n",
        "- Offering a clear interpretation of overfitting/underfitting behavior.\n",
        "- Trying multiple variants within your chosen idea.\n",
        "\n",
        "#### **Tier 3 ‚Äî Insightful extension (10 pts)**  \n",
        "Awarded for deeper insight or creative extensions, such as:\n",
        "- Incorporating **bi-clusters** or combining embedding-based ideas in a justified way.\n",
        "- Providing a well-reasoned explanation of how the representation influences classifier behavior.\n",
        "- Discussing limitations or failure modes.\n",
        "- Presenting a well-organized, thoughtful mini-report.\n",
        "\n",
        "**Total possible bonus: 30 points**\n",
        "\n"
      ],
      "metadata": {
        "id": "mtZd7ASGn4mY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöß What to do\n",
        "\n",
        "Choose **one main idea** and explore it.  \n",
        "You may try different variants of your chosen idea (e.g., number of clusters, weighting schemes, classifiers), and you **may** explore more than one idea if you wish, **but this is not required**.  As indicated by the bonus point struture, the bonus credit is awarded based on the **quality and depth** of your exploration, not the number of ideas attempted.\n",
        "\n",
        "You may use any classifier covered in this course.\n",
        "\n",
        "You are **not required** to achieve improved validation performance compared to IA3.\n",
        "\n",
        "Include your code (below) in this notebook and provide a brief report.\n",
        "---"
      ],
      "metadata": {
        "id": "JmUTujqXoBK2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQOAt24Db1kj"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ‚úçÔ∏è Bonus-part report.\n",
        "Your report should:\n",
        "\n",
        "1 Describes your idea (including any hyperparameters or variants you explored).\n",
        "\n",
        "2. Summarizes your results (training and validation performance, using AUC as the main metric).\n",
        "\n",
        "3. Interprets the results. For example: How did your representation affect overfitting/underfitting compared to BoW? Did the embedding-based features change the classifier‚Äôs behavior or sensitivity?"
      ],
      "metadata": {
        "id": "7HQo7IArTnp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your report goes here."
      ],
      "metadata": {
        "id": "omX3OAOtl3TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#running this code block will convert this notebook and its outputs into a pdf report.\n",
        "!jupyter nbconvert --to html /content/gdrive/MyDrive/Colab\\ Notebooks/IA4-2024.ipynb  # you might need to change this path to appropriate value to location your copy of the IA0 notebook\n",
        "\n",
        "input_html = '/content/gdrive/MyDrive/Colab Notebooks/IA4-2024.html' #you might need to change this path accordingly\n",
        "output_pdf = '/content/gdrive/MyDrive/Colab Notebooks/IA4output.pdf' #you might need to change this path or name accordingly\n",
        "\n",
        "# Convert HTML to PDF\n",
        "pdfkit.from_file(input_html, output_pdf)\n",
        "\n",
        "# Download the generated PDF\n",
        "files.download(output_pdf)"
      ],
      "metadata": {
        "id": "HOOl6Viq_f2v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
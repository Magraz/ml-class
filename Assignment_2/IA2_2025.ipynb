{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI534 IA2 - Logistic regression with L2 and L1 regularization\n",
        "\n",
        "**Submission**:  \n",
        "1. Your completed notebook in ipynb\n",
        "2. a PDF report that includes all code outputs and figures. You can use the code block at the end of the notebook to generate a PDF export of the notebook with the outputs for your report. However, if any figures or outputs are missing, you must either:\n",
        "\n",
        "\n",
        "* Manually add the missing figures to the PDF using a PDF editor or\n",
        "* Copy your notebook contents into a Word or Google Doc, insert the missing outputs there, and export that document as a PDF.\n",
        "\n",
        "\n",
        "**Overview.** In this assignment, we will implement and experiment with logistic regression with L2 and L1 regularization to predict whether a health insurance customer will purchase car insurance based on a set of features.\n",
        "\n",
        "You may modify the starter code as you see fit, including changing the signatures of functions and adding/removing helper functions. However, please make sure that your TA can understand what you are doing and why."
      ],
      "metadata": {
        "id": "zOK-6wmkFuv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First lets import the necessary packages."
      ],
      "metadata": {
        "id": "aP84hVzY97rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbconvert > /dev/null 2>&1\n",
        "!pip install pdfkit > /dev/null 2>&1\n",
        "!apt-get install -y wkhtmltopdf > /dev/null 2>&1\n",
        "import os\n",
        "import pdfkit\n",
        "import contextlib\n",
        "import sys\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# add more imports if necessary"
      ],
      "metadata": {
        "id": "mfC82i5Bw_Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 0. (2 pts) Loading data and perform feature normalization for numerical features\n",
        "\n",
        "---\n",
        "\n",
        "On canvas, we have provided three different data files for this assignment: IA2-train.csv (for training), IA2-dev.csv(for validation) and IA2-train-noisy.csv (for Part 3). Download them and upload them to your google drive. Then mount the google drive from your google colab notebook:\n"
      ],
      "metadata": {
        "id": "7Z0sSMySHB-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "train_path = '/content/gdrive/My Drive/AI534/IA2-train.csv' # DO NOT MODIFY THIS. Please make sure your data has this exact path\n",
        "val_path = '/content/gdrive/My Drive/AI534/IA2-dev.csv' # DO NOT MODIFY THIS. Please make sure your data has this exact path\n",
        "noisy_train_path = '/content/gdrive/My Drive/AI534/IA2-train-noisy.csv' # DO NOT MODIFY THIS. Please make sure your data has this exact path"
      ],
      "metadata": {
        "id": "E6K8DsDcwjS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d36d1dd-cf28-4ad5-cf9e-ec34d5690376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ##  Preprocessing.\n",
        " You have one additional preprocessing step to do, which is to perform feature normalization (z-score) for 3 numerical features (\"Age\", \"Annual_Premium\", \"Vintage\")."
      ],
      "metadata": {
        "id": "nsuGx9oFxRu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code goes here"
      ],
      "metadata": {
        "id": "9uTz8ldzx0EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 (35 pts) Logstic regression with L2 (Ridge) regularization\n",
        "\n",
        "For this part of the assignment, you will implement and experiment with Logistic regression with L2 regularization (Algorithm 1 in Assignment 2 Reference Information on canvas).\n"
      ],
      "metadata": {
        "id": "C4djL2J-By8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##    Impelement Logistic regression with L2 regularization\n",
        "Your implemented function should take the following inputs:\n",
        "1. The training data\n",
        "2. The regularization parameter $\\lambda$\n",
        "3. The learning rate\n",
        "4. Max iterations (recommend to start with 5000)\n",
        "5. Threshold for change in loss (this will be used for early stopping: if the change in loss is less than the threshold, it is considered to have converged. Please use a threhold of $10^{-7}$. )\n",
        "\n",
        "Your function should output the learned weight vector and the sequence of training losses, which will allow you to visualize the convergence process to ensure proper convergence. You should also implement a divergence detection, if the loss starts to diverge, terminate and raise an alarm.\n",
        "\n"
      ],
      "metadata": {
        "id": "T0edcBa0MtVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "sc2kX8JxXLiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Experiment with different regularization parameters\n",
        "For this part, you will run your L2 logistic regression on the training data with different regularization strengths $\\lambda\\in \\{10^i: i\\in[-5, 0]\\}$. This is the minimim range of values required for your exploration. You are encouraged to try additional intermediate or more extreme values if it helps you better analyze the results and answer the questions.\n",
        "\n",
        "**Learning Rate Tuning Guidelines:**\n",
        "The learning rate value ($\\gamma$)  will need to be adjusted depending on the value of $\\lambda$:\n",
        "*   For very small $\\lambda$ values (e.g.,$10^{-5}, 10^{-4}$), start with a  larger learning rate (e.g., $\\gamma=1$) .\n",
        "*   For moderate $\\lambda$ values like $10^{-3}$, try a smaller learning rate like $\\gamma = 0.1$.\n",
        "*   As $\\lambda$ increases further, continue decreasig the learning rate to maintain stable convergence.\n",
        "\n",
        "** Why smaller learning rate for larger $\\lambda$?**\n",
        "Stronger regularization amplifies the contribution of the penalty term in the gradient. Using the same learning rate across all $\\lambda$ values can lead to overly large update steps and unstable training. Adjusting the learning rate ensures smoother and more stable convergence.\n",
        "\n",
        "\n",
        "**What to complete here.**\n",
        "For each $\\lambda$ value:\n",
        "\n",
        "* Run your logistic regression until it converges(using your early stopping critierion).\n",
        "* Record the final weight vector, which will be used later.\n",
        "* Compute and record the training and validation accuracies\n",
        "* Summarize your results in a clear table that lists each $\\lambda$ value, the corresponding training accuracy, and validation accuracy. You will use this data in the next part to create your plots."
      ],
      "metadata": {
        "id": "DIn4qhdOftK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "t3PH0wZcNXxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Visualize and analyze $\\lambda$'s impact on training and validation accuracy\n",
        "\n",
        "Now, use the results from your experiments to visualize how model performance changes with the regularization strength. Plot both the training accuracy and validation accuracy of your L2 regularized logistic regression model as a function of $\\lambda$.\n",
        "* Use a logarithmic scale for the x-axis to represent $\\lambda$. Each tick mark on the x-axis should correspond to an integar $i$ and be labeled as $10^{i}$. This helps you clearly see performance trends across multiple orders of magnitude of $\\lambda$.  \n",
        "\n",
        "* Plot the two curves in the same figure, one for training accuracy and one for validation accuracy, using different colors for clarity.\n",
        "* Include a legend to indicate which curve corresponds to which accuracy measure, and label the axis clearly.\n",
        "\n",
        "Your final plot should make it easy to compare how regularization affects training vs. validation performance."
      ],
      "metadata": {
        "id": "IHbzTPFkNNp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "oW7FWwnKQ_Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 锔 Question\n",
        "\n",
        "(a) Which $\\lambda$ value leads to the best training and validation accuracy respectively? Which one should you use if you are to pick a model for deployment?\n",
        "\n",
        "(b) What trend do you observe for the training and validation accuracy respectively as we increae $\\lambda$? Provide your explanation for this observed trend.\n"
      ],
      "metadata": {
        "id": "0l6zfscrfBgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here**"
      ],
      "metadata": {
        "id": "otogNnOaTNCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Examine the impact on feature weights\n",
        "For each value of $\\lambda$, present the top five features based on the magnitude of their weights $|w_j|$, excluding the bias term $w_0$. Organize your results into a table. Each column should be dedicated to a specific $\\lambda$ value, and rows should indicate the rank of the feature. Ensure that each cell in the table contains both the feature name and its corresponding weight $w_j$.\n",
        "\n",
        "For example:\n",
        "\n",
        "| Rank  | $\\lambda = 10^{-4}$      | $\\lambda = 10^{-3}$      | $\\lambda = 10^{-2}$      |\n",
        "|-------|--------------------------|--------------------------|--------------------------|\n",
        "| 1     | feature_a, 0.8           | feature_b, 0.7           | feature_c, 0.6           |\n",
        "| 2     | feature_d, 0.7           | feature_e, 0.6           | feature_f, 0.5           |\n",
        "| 3     | feature_g, 0.6           | feature_h, 0.5           | feature_i, 0.4           |\n",
        "| 4     | feature_j, 0.5           | feature_k, 0.4           | feature_l, 0.3           |\n",
        "| 5     | feature_m, 0.4           | feature_n, 0.3           | feature_o, 0.2           |\n",
        "\n",
        "The easiest way is to create a dataframe for this table and print the dataframe."
      ],
      "metadata": {
        "id": "ZrZaz5OUNpyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Your code goes here"
      ],
      "metadata": {
        "id": "QhpNixB_Smdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 锔 Question\n",
        "\n",
        "1. Do you observe any difference is the top features with different $\\lambda$ values?\n",
        "2. Do you observe any difference in the weights of the top features for different $\\lambda$ values?\n",
        "3. Please provide your own explanation/interpretation of the observed differences."
      ],
      "metadata": {
        "id": "QMienw7hSpjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here**"
      ],
      "metadata": {
        "id": "qloa-NvMTKVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Examine the impact on sparsity of weights\n",
        "\n",
        "For each different value of $\\lambda$, compute the sparsity of the learned classifier as the number of feature weights that approxmately equal zero ($\\leq 10^{-6}$) and report the sparsity number for each $\\lambda$ value."
      ],
      "metadata": {
        "id": "K_rCHiS4TVPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "rkMFgQ38T96l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 锔Question\n",
        "\n",
        "1. When we have very small $\\lambda$ values (aka very weak regularization), does your learned model have zero weights for some features?  If so, why would it be that way?\n",
        "2. What trend do you observe for the sparsity of the model as we increase $\\lambda$? If we further increase $\\lambda$ to even larger values, what do you expect to happen to the sparsity value? Why?\n"
      ],
      "metadata": {
        "id": "1qgi8wxLsLue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here**"
      ],
      "metadata": {
        "id": "cUV1m5yeUJgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2. (38 pts) Logistic regression with L1 regularization.\n",
        "\n",
        "In this part, we will repeat the part 1 but with L1 regularization. Please refer to the algorithm 2 in the Assignment 2 Reference Information file for the details of the algorithm."
      ],
      "metadata": {
        "id": "3DXrFWL_up5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Impelement Logistic regression with L1 regularization\n",
        "**Requirment.**\n",
        "Implement Algorithm 2 (Proximal gradient descent for LASSO logistic regression) for L1 regularized Logistic Regress, described in the IA2 reference information sheet provided on Canvas. Your implemented function should take the following inputs:\n",
        "1. The training data\n",
        "2. The regularization parameter $\\lambda$\n",
        "3. The learning rate\n",
        "4. Max iterations (recommend to start with 5000)\n",
        "5. Threshold for change in loss (this will be used for early stopping: if the change in loss is less than the threshold, it is considered to have converged. Please use a threshold of $10^{-7}$. )\n",
        "\n",
        "Your function should output the learned weight vector and the sequence of losses so that you can visualize the convergence process. You should also implement a divergence detection, if the loss starts to diverge, terminate and raise an alarm.\n"
      ],
      "metadata": {
        "id": "tjXJLpLNvLY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here."
      ],
      "metadata": {
        "id": "tZOt2YfzvY2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Experiment with different regularization parameters\n",
        "For this part, you will need to apply your L1 logistic regression algorithm on the training data with different regularization parameters $\\lambda\\in \\{10^i: i\\in[-6, -1]\\}$. You are encouraged to experiment with more extreme or in-between values if it helps you in answering the questions. But be advised using larger $\\lambda$ values in this case makes it difficult to converge.\n",
        "\n",
        "**Learning Rate Tuning Guidelines:**\n",
        "For L1 regularization, I recommend starting with $\\gamma = 2$ for very small $\\lambda$s like $10^{-6}, 10^{-5}$, and decreasing the learning for larger $\\lambda$ values.\n",
        "\n",
        "**What to complete here.**\n",
        "For each $\\lambda$ value:\n",
        "* Run your L1 regularized logistic regression until it converges(using your early stopping critierion).\n",
        "* Record the final weight vector, which will be used later.\n",
        "* Compute and record the training and validation accuracies\n",
        "* Summarize your results in a clear table that lists each  位  value, the corresponding training accuracy, and validation accuracy. You will use this data in the next part to create your plots."
      ],
      "metadata": {
        "id": "o-ALab8qvbYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "QruW29Nn1zFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Visualize and analyze  位 's impact on training and validation accuracy\n",
        "Now, use the results from your experiments to visualize how model performance changes with the regularization strength. Plot both the training accuracy and validation accuracy of your L1 regularized logistic regression model as a function of $\\lambda$.\n",
        "* Use a logarithmic scale for the x-axis to represent $\\lambda$. Each tick mark on the x-axis should correspond to an integar $i$ and be labeled as $10^{i}$. This helps you clearly see performance trends across multiple orders of magnitude of $\\lambda$.  \n",
        "\n",
        "* Plot the two curves in the same figure, one for training accuracy and one for validation accuracy, using different colors for clarity.\n",
        "* Include a legend to indicate which curve corresponds to which accuracy measure, and label the axis clearly.\n",
        "\n",
        "Your final plot should make it easy to compare how regularization affects training vs. validation performance."
      ],
      "metadata": {
        "id": "WP15VwDh11e6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "njX32s4Q3xdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 锔Question\n",
        "Based on your results, answer the following questions.\n",
        "\n",
        "a. For L1 LR, which $\\lambda$ value leads to the best training and validatoin accuracy respectively? If you were to select a model for deployment, which\n",
        "$\\lambda$ would you choose, and why?\n",
        "\n",
        "b. What trend do you observe for the training and validation accuracy respectively as we increae $\\lambda$? Explain this trend and provide an intuitive reasoning for why it occurs.\n",
        "\n",
        "c. Comparing L1 and L2 regularized logistic regression, which one is more sensitive to the choice of the regularization parameter? An algorithm is considered sensitive if its performance or learned parameters change significantly when the regularization strength changes.\n",
        "Since your experiments may have used a smaller range of $\\lambda$ values for L1 regularization due to convergence concerns, you may use an off-the-shelf implementation (e.g., from scikit-learn) to explore this comparison more fully. Reflect on the differences you observe (or expect to observe) between L1 and L2 in terms of their sensitivity, and explain why.\n"
      ],
      "metadata": {
        "id": "dKpnfLh230m4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here**"
      ],
      "metadata": {
        "id": "aH54ckNf5u9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Examine the impact on feature weights\n",
        "For each value of $\\lambda$, present the top five features based on the magnitude of their weights $|w_j|$, excluding the bias term $w_0$. Organize your results into a table. Each column should be dedicated to a specific $\\lambda$ value, and rows should indicate the rank of the feature. Ensure that each cell in the table contains both the feature name and its corresponding weight $w_j$.\n",
        "\n",
        "For example:\n",
        "\n",
        "| Rank  | $\\lambda = 10^{-4}$      | $\\lambda = 10^{-3}$      | $\\lambda = 10^{-2}$      |\n",
        "|-------|--------------------------|--------------------------|--------------------------|\n",
        "| 1     | feature_a, 0.8           | feature_b, 0.7           | feature_c, 0.6           |\n",
        "| 2     | feature_d, 0.7           | feature_e, 0.6           | feature_f, 0.5           |\n",
        "| 3     | feature_g, 0.6           | feature_h, 0.5           | feature_i, 0.4           |\n",
        "| 4     | feature_j, 0.5           | feature_k, 0.4           | feature_l, 0.3           |\n",
        "| 5     | feature_m, 0.4           | feature_n, 0.3           | feature_o, 0.2           |"
      ],
      "metadata": {
        "id": "9Czbhsql7x7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "1dWQcu5J76ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##锔Question\n",
        "\n",
        "1. Do you observe any difference is the top features with different $\\lambda$ values?\n",
        "2. Do you observe any difference in the weights of the top features for different $\\lambda$ values?\n",
        "3. Please provide your own explanation/interpretation of the observed differences.\n",
        "4. What are some differences for this part of the results comparing L1 and L2 regularization? Provide your own explanation for such differences.\n"
      ],
      "metadata": {
        "id": "ibv19RIT791s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here**"
      ],
      "metadata": {
        "id": "7Azz6YUb8epJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Examine the impact on sparsity of weights\n",
        "\n",
        "For each different value of $\\lambda$, compute the sparsity of the learned L1 regularized logistic regression classifier as the number of feature weights that approxmately equal zero ($\\leq 10^{-6}$) and report the sparsity number for each $\\lambda$ value."
      ],
      "metadata": {
        "id": "pbEGWb2r8lT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "Fr7Z43bH8veX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 锔Question\n",
        "\n",
        "1. What trend do you observe for the sparsity of the L1 regularized model as we change $\\lambda$? If we further increase $\\lambda$, what do you expect? Why?\n",
        "2. What are some differences for this part of the results comparing L1 and L2 regularization? Provide your own explanation for such differences.\n"
      ],
      "metadata": {
        "id": "HOW68szl8yNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here**"
      ],
      "metadata": {
        "id": "2B_am07d9Ei7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3. (15 pts) Impact of Noise in Training Data\n",
        "For this part, you will be training both L1 and L2 logistic regression models using the noisy training data (IA2-train-noisy.csv)."
      ],
      "metadata": {
        "id": "u84B_DIRYdRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Experiment L1 and L2 Logistic Regression on Noisy Training data.\n",
        "This experiment follows the same structure as Parts 1 and 2, so you can reuse your previous codejust provide the noisy training data (IA2-train-noisy.csv) as input.\n",
        "You may also use an off-the-shelf implementation such as sklearn.linear_model.LogisticRegression, which will typically be more efficient than your custom version.\n",
        "If you choose to use scikit-learn:\n",
        "* Set the penalty argument to 'l1' or 'l2'.\n",
        "\n",
        "* Use the 'liblinear' or 'saga' solver (required for L1).\n",
        "\n",
        "* Remember that scikit-learns regularization parameter is defined as C=$\\frac{1}{\\lambda}$, so smaller  values correspond to stronger regularization.\n",
        "\n",
        "For each regularization type:\n",
        "\n",
        "* Train the model for each 位 value as specified in each part. (For L1, you can exclude $\\lambda=0.1$, which can be difficult to converge)\n",
        "\n",
        "* Record the accuracies on the noisy training data and the clean validation data for each $\\lambda$ value.\n",
        "\n",
        "* Plot both accuracies as functions of 位 on a logarithmic x-axis, using distinct colors and a clear legend.\n",
        "\n",
        "* Both curves (training and validation) should appear on the same figure for easy comparison.\n",
        "\n",
        "Your plots should clearly illustrate how model performance changes with different regularization strengths under noisy conditions."
      ],
      "metadata": {
        "id": "vTFA420-pNJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "\n"
      ],
      "metadata": {
        "id": "FC1GE-OkYPwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 锔Question\n",
        "Your experiments should reveal that when trained with the noisy data, both L1 and L2 regulated logistic regression have substantially reduced training accuracies, but only a small drop in validation accuracy.\n",
        "\n",
        "Here I provide two possible explanations for this phenonmenon:\n",
        "1. This is due to the use of regularization, which limits the model's ability to overfit to the noise.\n",
        "2. This is due to the simplicity of the model, which prevents overfitting even without regularization.\n",
        "\n",
        "Which explanation better accounts for your results, and why? Support your reasoning using evidence from your plots (e.g., how accuracy changes with\n",
        "位, or differences between L1 and L2).\n",
        "\n",
        "Design a brief experiment to figure which explanation fits in this particular case. between these two explanations. Describe what you would vary, what you would keep fixed, and what outcome would support each hypothesis. (You only need to describe the experiment, not run it.)"
      ],
      "metadata": {
        "id": "OL-EIkrVizZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer goes here.**"
      ],
      "metadata": {
        "id": "EmKd-MT8u34x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4. (10 pts) In-class competition\n",
        "We will host a in-class competition using the IA2 data. To participate in this competition, use the following link:\n",
        "https://www.kaggle.com/competitions/ai534-ia2-25\n",
        "\n",
        "**Model restriction.** For this competition, you are required to use logistic regression models. You are welcome to use off-the-shelf implementations, such as sklearn.linear_model.LogisticRegression.\n",
        "\n",
        "**Exploration encouraged.** To improve your model's performance, you may:\n",
        "\n",
        "* Perform feature engineering (create, modify, or combine features).\n",
        "* Manipulate the data (upsample or downsample the training set)\n",
        "* Experiment with hyperparameter tuning.\n",
        "* Try different regularization methods (e.g., L1, L2).\n",
        "\n",
        "\n",
        "**Team work.** You should continue working in the same team for this competition. The training and validation data provided on the kaggle site are the same as the IA2 assignment.\n",
        "\n",
        "**Evaluation** To participate, you will apply your trained/tuned model to the test data provided on kaggle (which does not contian the response column), and submit prediction files to be scored, based on prediction accuracy.\n",
        "\n",
        "There are two parts to the score you will see on kaggle. The performance reported on the public leaderboard and a score reported on the private leaderboard. The public leader board scores are visible through out the competition and you can use it as an external validation to help you refine your model design and tune the model. The private leader board scores are evaluated using a separate set of test data as the final performance evaluation and will be released only after the competition is closed.\n",
        "\n",
        "**Points and bonus points.** You will get the full 10 points if you\n",
        "\n",
        "* participate in the competition (successful submissions)\n",
        "\n",
        "* achieve non-trivial performance (outperform some simple baseline)\n",
        "\n",
        "* complete the report on the competition below.\n",
        "\n",
        "You will get 3 **nonus points** if your team scored top 3 on the private leader board, or entered the largest number of unique submissions (unique sores).\n",
        "\n",
        "No late submission. The competition will be closed at 11:59 pm of the due date."
      ],
      "metadata": {
        "id": "Cu4q7E3CJTwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##锔 Report on the Kaggle competition\n",
        "1. Team name:\n",
        "2. Exploration Summary: Brief describe the approches you tried.\n",
        "3. Most Impactful Change: Which exploration led to the most performance improvement, and why do you think it helped?"
      ],
      "metadata": {
        "id": "nUBFNWdVvP6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#running this code block will convert this notebook and its outputs into a pdf report.\n",
        "!jupyter nbconvert --to html /content/gdrive/MyDrive/Colab\\ Notebooks/IA2-2024.ipynb  # you might need to change this path to appropriate value to location your copy of the IA0 notebook\n",
        "\n",
        "input_html = '/content/gdrive/MyDrive/Colab Notebooks/IA2-2024.html' #you might need to change this path accordingly\n",
        "output_pdf = '/content/gdrive/MyDrive/Colab Notebooks/IA2output.pdf' #you might need to change this path or name accordingly\n",
        "\n",
        "# Convert HTML to PDF\n",
        "pdfkit.from_file(input_html, output_pdf)\n",
        "\n",
        "# Download the generated PDF\n",
        "files.download(output_pdf)"
      ],
      "metadata": {
        "id": "VBJ-Uzzk7yIC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}